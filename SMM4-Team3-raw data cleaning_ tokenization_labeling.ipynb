{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change it if you use the different path\n",
    "tweet_file = pd.read_csv('data/task3_training.tsv',delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2246, 54)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_file.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet text cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discrad the unrelevant columns\n",
    "tweet_file_2 = tweet_file.iloc[:,:9]\n",
    "\n",
    "# change all the text to lowercase\n",
    "tweet_file_2['cleaned_tweet'] = tweet_file_2['tweet'].apply(str.lower)\n",
    "\n",
    "# remove @ mentions (remove the first '@' and following username)\n",
    "tweet_file_2['cleaned_tweet'] = tweet_file_2['cleaned_tweet'].replace(to_replace=r'@([_A-Za-z]+[A-Za-z0-9-_]+)', value='', regex=True)\n",
    "\n",
    "# remove all other non-standard characters (emoji, #, @....) ( For this task I thought '@'' and '#' are useless )\n",
    "tweet_file_2['cleaned_tweet'] = tweet_file_2['cleaned_tweet'].replace(to_replace=r\"\"\"([^A-Za-z0-9 .,\\-'\"\\/?!\\\\()=])+\"\"\", value='', regex=True)\n",
    "\n",
    "# remove urls\n",
    "tweet_file_2['cleaned_tweet'] = tweet_file_2['cleaned_tweet'].replace(to_replace=r'http\\S+', value='', regex=True)\n",
    "\n",
    "# Expand contractions\n",
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "tweet_file_2['cleaned_tweet'] = tweet_file_2['cleaned_tweet'].apply(decontracted)\n",
    "\n",
    "\n",
    "# Let me know if you think more cleaning processes need to be done!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization & Lemmetization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As reported by the papers, different tokenizers may have different performance so I tried different tokenizers,\n",
    "# their performance should be decided by the classification results\n",
    "\n",
    "# Trial 1, used the nltk.word_tokenizer\n",
    "# import nltk\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# word_lemmatizer = WordNetLemmatizer()\n",
    "# def tokenize_lemmatize(twitter):\n",
    "#     tokens = nltk.word_tokenize(twitter)\n",
    "#     tokens_lemmetized = []\n",
    "#     for token in tokens:\n",
    "#         tokens_lemmetized.append(word_lemmatizer.lemmatize(token))\n",
    "#     lemmed_twitter = ' '.join(tokens_lemmetized)\n",
    "#     return lemmed_twitter\n",
    "\n",
    "# tweet_tokenization_1 = tweet_file_2\n",
    "# tweet_tokenization_1['cleaned_tweet'] = tweet_file_2['cleaned_tweet'].apply(tokenize_lemmatize)\n",
    "\n",
    "# tweet_tokenization_1['cleaned_tweet']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just tokenization, no lemmatization of twitters\n",
    "\n",
    "There will be some matching issue after lemmatization, so we just used tokenization of twitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>begin</th>\n",
       "      <th>end</th>\n",
       "      <th>type</th>\n",
       "      <th>extraction</th>\n",
       "      <th>drug</th>\n",
       "      <th>tweet</th>\n",
       "      <th>meddra_code</th>\n",
       "      <th>meddra_term</th>\n",
       "      <th>cleaned_tweet</th>\n",
       "      <th>tokennized_cleaned_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>331187619096588288</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ofloxacin</td>\n",
       "      <td>@seefisch:oral drugs for pyelonephritis:ciprof...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>oral drugs for pyelonephritisciprofloxacin lev...</td>\n",
       "      <td>[oral, drugs, for, pyelonephritisciprofloxacin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>332227554956161024</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>trazodone</td>\n",
       "      <td>happy for wellbutrin; has similar effects as a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>happy for wellbutrin has similar effects as ad...</td>\n",
       "      <td>[happy, for, wellbutrin, has, similar, effects...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>332448217490944000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>lamotrigine</td>\n",
       "      <td>@stilgarg i'm ok ty have an official diagnosis...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>i am ok ty have an official diagnosis of bipo...</td>\n",
       "      <td>[i, am, ok, ty, have, an, official, diagnosis,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>332977955754110976</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cymbalta</td>\n",
       "      <td>i'm soo depressed cymbalta couldn't help me .</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>i am soo depressed cymbalta could not help me .</td>\n",
       "      <td>[i, am, soo, depressed, cymbalta, could, not, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>333674203331051520</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>seroquel</td>\n",
       "      <td>time for my daily afternoon relaxation ritual ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>time for my daily afternoon relaxation ritual ...</td>\n",
       "      <td>[time, for, my, daily, afternoon, relaxation, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id  begin  end type extraction         drug  \\\n",
       "0  331187619096588288    NaN  NaN  NaN        NaN    ofloxacin   \n",
       "1  332227554956161024    NaN  NaN  NaN        NaN    trazodone   \n",
       "2  332448217490944000    NaN  NaN  NaN        NaN  lamotrigine   \n",
       "3  332977955754110976    NaN  NaN  NaN        NaN     cymbalta   \n",
       "4  333674203331051520    NaN  NaN  NaN        NaN     seroquel   \n",
       "\n",
       "                                               tweet  meddra_code meddra_term  \\\n",
       "0  @seefisch:oral drugs for pyelonephritis:ciprof...          NaN         NaN   \n",
       "1  happy for wellbutrin; has similar effects as a...          NaN         NaN   \n",
       "2  @stilgarg i'm ok ty have an official diagnosis...          NaN         NaN   \n",
       "3      i'm soo depressed cymbalta couldn't help me .          NaN         NaN   \n",
       "4  time for my daily afternoon relaxation ritual ...          NaN         NaN   \n",
       "\n",
       "                                       cleaned_tweet  \\\n",
       "0  oral drugs for pyelonephritisciprofloxacin lev...   \n",
       "1  happy for wellbutrin has similar effects as ad...   \n",
       "2   i am ok ty have an official diagnosis of bipo...   \n",
       "3    i am soo depressed cymbalta could not help me .   \n",
       "4  time for my daily afternoon relaxation ritual ...   \n",
       "\n",
       "                            tokennized_cleaned_tweet  \n",
       "0  [oral, drugs, for, pyelonephritisciprofloxacin...  \n",
       "1  [happy, for, wellbutrin, has, similar, effects...  \n",
       "2  [i, am, ok, ty, have, an, official, diagnosis,...  \n",
       "3  [i, am, soo, depressed, cymbalta, could, not, ...  \n",
       "4  [time, for, my, daily, afternoon, relaxation, ...  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "tweet_tokenization_2 = tweet_file_2.copy()\n",
    "tokenize = nltk.word_tokenize\n",
    "tweet_tokenization_2['tokennized_cleaned_tweet'] = tweet_tokenization_2['cleaned_tweet'].apply(tokenize)\n",
    "tweet_tokenization_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>begin</th>\n",
       "      <th>end</th>\n",
       "      <th>type</th>\n",
       "      <th>extraction</th>\n",
       "      <th>drug</th>\n",
       "      <th>tweet</th>\n",
       "      <th>meddra_code</th>\n",
       "      <th>meddra_term</th>\n",
       "      <th>cleaned_tweet</th>\n",
       "      <th>tokennized_cleaned_tweet</th>\n",
       "      <th>extraction_token</th>\n",
       "      <th>labeled_sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>331187619096588288</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>ofloxacin</td>\n",
       "      <td>@seefisch:oral drugs for pyelonephritis:ciprof...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>oral drugs for pyelonephritisciprofloxacin lev...</td>\n",
       "      <td>[oral, drugs, for, pyelonephritisciprofloxacin...</td>\n",
       "      <td>[None]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>332227554956161024</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>trazodone</td>\n",
       "      <td>happy for wellbutrin; has similar effects as a...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>happy for wellbutrin has similar effects as ad...</td>\n",
       "      <td>[happy, for, wellbutrin, has, similar, effects...</td>\n",
       "      <td>[None]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>332448217490944000</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>lamotrigine</td>\n",
       "      <td>@stilgarg i'm ok ty have an official diagnosis...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>i am ok ty have an official diagnosis of bipo...</td>\n",
       "      <td>[i, am, ok, ty, have, an, official, diagnosis,...</td>\n",
       "      <td>[None]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>332977955754110976</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>cymbalta</td>\n",
       "      <td>i'm soo depressed cymbalta couldn't help me .</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>i am soo depressed cymbalta could not help me .</td>\n",
       "      <td>[i, am, soo, depressed, cymbalta, could, not, ...</td>\n",
       "      <td>[None]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>333674203331051520</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>seroquel</td>\n",
       "      <td>time for my daily afternoon relaxation ritual ...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>time for my daily afternoon relaxation ritual ...</td>\n",
       "      <td>[time, for, my, daily, afternoon, relaxation, ...</td>\n",
       "      <td>[None]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id begin   end  type extraction         drug  \\\n",
       "0  331187619096588288  None  None  None       None    ofloxacin   \n",
       "1  332227554956161024  None  None  None       None    trazodone   \n",
       "2  332448217490944000  None  None  None       None  lamotrigine   \n",
       "3  332977955754110976  None  None  None       None     cymbalta   \n",
       "4  333674203331051520  None  None  None       None     seroquel   \n",
       "\n",
       "                                               tweet meddra_code meddra_term  \\\n",
       "0  @seefisch:oral drugs for pyelonephritis:ciprof...        None        None   \n",
       "1  happy for wellbutrin; has similar effects as a...        None        None   \n",
       "2  @stilgarg i'm ok ty have an official diagnosis...        None        None   \n",
       "3      i'm soo depressed cymbalta couldn't help me .        None        None   \n",
       "4  time for my daily afternoon relaxation ritual ...        None        None   \n",
       "\n",
       "                                       cleaned_tweet  \\\n",
       "0  oral drugs for pyelonephritisciprofloxacin lev...   \n",
       "1  happy for wellbutrin has similar effects as ad...   \n",
       "2   i am ok ty have an official diagnosis of bipo...   \n",
       "3    i am soo depressed cymbalta could not help me .   \n",
       "4  time for my daily afternoon relaxation ritual ...   \n",
       "\n",
       "                            tokennized_cleaned_tweet extraction_token  \\\n",
       "0  [oral, drugs, for, pyelonephritisciprofloxacin...           [None]   \n",
       "1  [happy, for, wellbutrin, has, similar, effects...           [None]   \n",
       "2  [i, am, ok, ty, have, an, official, diagnosis,...           [None]   \n",
       "3  [i, am, soo, depressed, cymbalta, could, not, ...           [None]   \n",
       "4  [time, for, my, daily, afternoon, relaxation, ...           [None]   \n",
       "\n",
       "  labeled_sequence  \n",
       "0                   \n",
       "1                   \n",
       "2                   \n",
       "3                   \n",
       "4                   "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fill the NAN with None \n",
    "tweet_tokenization_2.fillna('None',inplace=True)\n",
    "\n",
    "# tokenize the already extracted label(AEs)\n",
    "tweet_tokenization_2['extraction_token'] = tweet_tokenization_2['extraction'].apply(tokenize)\n",
    "labeled_dataset_t = tweet_tokenization_2.copy()\n",
    "\n",
    "# Create a empty column 'label' for sequence labeling \n",
    "labeled_dataset_t['labeled_sequence'] =''\n",
    "labeled_dataset_t.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>begin</th>\n",
       "      <th>end</th>\n",
       "      <th>type</th>\n",
       "      <th>extraction</th>\n",
       "      <th>drug</th>\n",
       "      <th>tweet</th>\n",
       "      <th>meddra_code</th>\n",
       "      <th>meddra_term</th>\n",
       "      <th>cleaned_tweet</th>\n",
       "      <th>tokennized_cleaned_tweet</th>\n",
       "      <th>extraction_token</th>\n",
       "      <th>labeled_sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>331187619096588288</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>ofloxacin</td>\n",
       "      <td>@seefisch:oral drugs for pyelonephritis:ciprof...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>oral drugs for pyelonephritisciprofloxacin lev...</td>\n",
       "      <td>[oral, drugs, for, pyelonephritisciprofloxacin...</td>\n",
       "      <td>[None]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>332227554956161024</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>trazodone</td>\n",
       "      <td>happy for wellbutrin; has similar effects as a...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>happy for wellbutrin has similar effects as ad...</td>\n",
       "      <td>[happy, for, wellbutrin, has, similar, effects...</td>\n",
       "      <td>[None]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>332448217490944000</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>lamotrigine</td>\n",
       "      <td>@stilgarg i'm ok ty have an official diagnosis...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>i am ok ty have an official diagnosis of bipo...</td>\n",
       "      <td>[i, am, ok, ty, have, an, official, diagnosis,...</td>\n",
       "      <td>[None]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>332977955754110976</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>cymbalta</td>\n",
       "      <td>i'm soo depressed cymbalta couldn't help me .</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>i am soo depressed cymbalta could not help me .</td>\n",
       "      <td>[i, am, soo, depressed, cymbalta, could, not, ...</td>\n",
       "      <td>[None]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>333674203331051520</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>seroquel</td>\n",
       "      <td>time for my daily afternoon relaxation ritual ...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>time for my daily afternoon relaxation ritual ...</td>\n",
       "      <td>[time, for, my, daily, afternoon, relaxation, ...</td>\n",
       "      <td>[None]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id begin   end  type extraction         drug  \\\n",
       "0  331187619096588288  None  None  None       None    ofloxacin   \n",
       "1  332227554956161024  None  None  None       None    trazodone   \n",
       "2  332448217490944000  None  None  None       None  lamotrigine   \n",
       "3  332977955754110976  None  None  None       None     cymbalta   \n",
       "4  333674203331051520  None  None  None       None     seroquel   \n",
       "\n",
       "                                               tweet meddra_code meddra_term  \\\n",
       "0  @seefisch:oral drugs for pyelonephritis:ciprof...        None        None   \n",
       "1  happy for wellbutrin; has similar effects as a...        None        None   \n",
       "2  @stilgarg i'm ok ty have an official diagnosis...        None        None   \n",
       "3      i'm soo depressed cymbalta couldn't help me .        None        None   \n",
       "4  time for my daily afternoon relaxation ritual ...        None        None   \n",
       "\n",
       "                                       cleaned_tweet  \\\n",
       "0  oral drugs for pyelonephritisciprofloxacin lev...   \n",
       "1  happy for wellbutrin has similar effects as ad...   \n",
       "2   i am ok ty have an official diagnosis of bipo...   \n",
       "3    i am soo depressed cymbalta could not help me .   \n",
       "4  time for my daily afternoon relaxation ritual ...   \n",
       "\n",
       "                            tokennized_cleaned_tweet extraction_token  \\\n",
       "0  [oral, drugs, for, pyelonephritisciprofloxacin...           [None]   \n",
       "1  [happy, for, wellbutrin, has, similar, effects...           [None]   \n",
       "2  [i, am, ok, ty, have, an, official, diagnosis,...           [None]   \n",
       "3  [i, am, soo, depressed, cymbalta, could, not, ...           [None]   \n",
       "4  [time, for, my, daily, afternoon, relaxation, ...           [None]   \n",
       "\n",
       "                                    labeled_sequence  \n",
       "0   [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]  \n",
       "1  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "2  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "3                     [O, O, O, O, O, O, O, O, O, O]  \n",
       "4  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# labeling the twitter, the label for begining of adverse effects is \"B\", other part of ADR is 'I' and the rest of twitter are 'O'\n",
    "def sequence_check(test):\n",
    "    for i in range(1,len(test)-1):\n",
    "        if test[i-1:i+2] == ['O', 'I', 'O']:\n",
    "            test[i] = 'O'\n",
    "    return test\n",
    "def sequence_label(sequence,extraction):\n",
    "    labels = []\n",
    "    for i in sequence:\n",
    "        if i in extraction:\n",
    "            if i == extraction[0]:\n",
    "                labels.append(\"B\")\n",
    "            else:\n",
    "                labels.append(\"I\")\n",
    "        else:\n",
    "            labels.append('O')\n",
    "    sequence_check(labels)\n",
    "    return labels\n",
    "\n",
    "\n",
    "for i in range(0, len(labeled_dataset)):\n",
    "    labeled_dataset_t['labeled_sequence'][i] = sequence_label(labeled_dataset_t['tokennized_cleaned_tweet'][i],labeled_dataset_t['extraction_token'][i])\n",
    "\n",
    "labeled_dataset_t.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/Training_data.txt\", \"a\") as f:\n",
    "    for i in range(0,labeled_dataset_t.shape[0]):\n",
    "        for k in range(0,len(labeled_dataset_t['tokennized_cleaned_tweet'][i])):\n",
    "            line = labeled_dataset_t['tokennized_cleaned_tweet'][i][k] + \"\\t\"+ labeled_dataset_t['labeled_sequence'][i][k]\n",
    "            print(line,file=f)\n",
    "        print('\\n',file=f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2246, 13)\n",
      "782\n"
     ]
    }
   ],
   "source": [
    "print(labeled_dataset_t.shape)\n",
    "print(sum(labeled_dataset_t['meddra_code']==\"None\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_file = pd.read_csv('data/task3_validation.tsv',delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(560, 9)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_file.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>begin</th>\n",
       "      <th>end</th>\n",
       "      <th>type</th>\n",
       "      <th>extraction</th>\n",
       "      <th>drug</th>\n",
       "      <th>tweet</th>\n",
       "      <th>meddra_code</th>\n",
       "      <th>meddra_term</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>332317478170546176</td>\n",
       "      <td>28.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>ADR</td>\n",
       "      <td>allergies</td>\n",
       "      <td>avelox</td>\n",
       "      <td>do you have any medication allergies? \"asthma!...</td>\n",
       "      <td>10013661.0</td>\n",
       "      <td>drug allergy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>347806215776116737</td>\n",
       "      <td>31.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>ADR</td>\n",
       "      <td>HURT YOUR Liver</td>\n",
       "      <td>avelox</td>\n",
       "      <td>@ashleylvivian if #avelox has hurt your liver,...</td>\n",
       "      <td>10024668.0</td>\n",
       "      <td>liver damage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>350336129817509888</td>\n",
       "      <td>48.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>ADR</td>\n",
       "      <td>AD</td>\n",
       "      <td>baclofen</td>\n",
       "      <td>apparently, baclofen greatly exacerbates the \"...</td>\n",
       "      <td>10003731.0</td>\n",
       "      <td>attention deficit disorder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>350336129817509888</td>\n",
       "      <td>88.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>ADR</td>\n",
       "      <td>focus</td>\n",
       "      <td>baclofen</td>\n",
       "      <td>apparently, baclofen greatly exacerbates the \"...</td>\n",
       "      <td>10003738.0</td>\n",
       "      <td>attention impaired</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>332540699692130304</td>\n",
       "      <td>11.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>ADR</td>\n",
       "      <td>died</td>\n",
       "      <td>cipro</td>\n",
       "      <td>pt of mine died from cipro rt @ciproispoison: ...</td>\n",
       "      <td>10011906.0</td>\n",
       "      <td>death</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id  begin   end type       extraction      drug  \\\n",
       "0  332317478170546176   28.0  37.0  ADR        allergies    avelox   \n",
       "1  347806215776116737   31.0  46.0  ADR  HURT YOUR Liver    avelox   \n",
       "2  350336129817509888   48.0  50.0  ADR               AD  baclofen   \n",
       "3  350336129817509888   88.0  93.0  ADR            focus  baclofen   \n",
       "4  332540699692130304   11.0  15.0  ADR             died     cipro   \n",
       "\n",
       "                                               tweet  meddra_code  \\\n",
       "0  do you have any medication allergies? \"asthma!...   10013661.0   \n",
       "1  @ashleylvivian if #avelox has hurt your liver,...   10024668.0   \n",
       "2  apparently, baclofen greatly exacerbates the \"...   10003731.0   \n",
       "3  apparently, baclofen greatly exacerbates the \"...   10003738.0   \n",
       "4  pt of mine died from cipro rt @ciproispoison: ...   10011906.0   \n",
       "\n",
       "                  meddra_term  \n",
       "0                drug allergy  \n",
       "1                liver damage  \n",
       "2  attention deficit disorder  \n",
       "3          attention impaired  \n",
       "4                       death  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_file_2 = validation_file.copy()\n",
    "validation_file_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>begin</th>\n",
       "      <th>end</th>\n",
       "      <th>type</th>\n",
       "      <th>extraction</th>\n",
       "      <th>drug</th>\n",
       "      <th>tweet</th>\n",
       "      <th>meddra_code</th>\n",
       "      <th>meddra_term</th>\n",
       "      <th>cleaned_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>332317478170546176</td>\n",
       "      <td>28.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>ADR</td>\n",
       "      <td>allergies</td>\n",
       "      <td>avelox</td>\n",
       "      <td>do you have any medication allergies? \"asthma!...</td>\n",
       "      <td>10013661.0</td>\n",
       "      <td>drug allergy</td>\n",
       "      <td>do you have any medication allergies? \"asthma!...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>347806215776116737</td>\n",
       "      <td>31.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>ADR</td>\n",
       "      <td>HURT YOUR Liver</td>\n",
       "      <td>avelox</td>\n",
       "      <td>@ashleylvivian if #avelox has hurt your liver,...</td>\n",
       "      <td>10024668.0</td>\n",
       "      <td>liver damage</td>\n",
       "      <td>if avelox has hurt your liver, avoid tylenol ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>350336129817509888</td>\n",
       "      <td>48.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>ADR</td>\n",
       "      <td>AD</td>\n",
       "      <td>baclofen</td>\n",
       "      <td>apparently, baclofen greatly exacerbates the \"...</td>\n",
       "      <td>10003731.0</td>\n",
       "      <td>attention deficit disorder</td>\n",
       "      <td>apparently, baclofen greatly exacerbates the \"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>350336129817509888</td>\n",
       "      <td>88.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>ADR</td>\n",
       "      <td>focus</td>\n",
       "      <td>baclofen</td>\n",
       "      <td>apparently, baclofen greatly exacerbates the \"...</td>\n",
       "      <td>10003738.0</td>\n",
       "      <td>attention impaired</td>\n",
       "      <td>apparently, baclofen greatly exacerbates the \"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>332540699692130304</td>\n",
       "      <td>11.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>ADR</td>\n",
       "      <td>died</td>\n",
       "      <td>cipro</td>\n",
       "      <td>pt of mine died from cipro rt @ciproispoison: ...</td>\n",
       "      <td>10011906.0</td>\n",
       "      <td>death</td>\n",
       "      <td>pt of mine died from cipro rt   if only more d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id  begin   end type       extraction      drug  \\\n",
       "0  332317478170546176   28.0  37.0  ADR        allergies    avelox   \n",
       "1  347806215776116737   31.0  46.0  ADR  HURT YOUR Liver    avelox   \n",
       "2  350336129817509888   48.0  50.0  ADR               AD  baclofen   \n",
       "3  350336129817509888   88.0  93.0  ADR            focus  baclofen   \n",
       "4  332540699692130304   11.0  15.0  ADR             died     cipro   \n",
       "\n",
       "                                               tweet  meddra_code  \\\n",
       "0  do you have any medication allergies? \"asthma!...   10013661.0   \n",
       "1  @ashleylvivian if #avelox has hurt your liver,...   10024668.0   \n",
       "2  apparently, baclofen greatly exacerbates the \"...   10003731.0   \n",
       "3  apparently, baclofen greatly exacerbates the \"...   10003738.0   \n",
       "4  pt of mine died from cipro rt @ciproispoison: ...   10011906.0   \n",
       "\n",
       "                  meddra_term  \\\n",
       "0                drug allergy   \n",
       "1                liver damage   \n",
       "2  attention deficit disorder   \n",
       "3          attention impaired   \n",
       "4                       death   \n",
       "\n",
       "                                       cleaned_tweet  \n",
       "0  do you have any medication allergies? \"asthma!...  \n",
       "1   if avelox has hurt your liver, avoid tylenol ...  \n",
       "2  apparently, baclofen greatly exacerbates the \"...  \n",
       "3  apparently, baclofen greatly exacerbates the \"...  \n",
       "4  pt of mine died from cipro rt   if only more d...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change all the text to lowercase\n",
    "validation_file_2['cleaned_tweet'] = validation_file_2['tweet'].apply(str.lower)\n",
    "\n",
    "# remove @ mentions (remove the first '@' and following username)\n",
    "validation_file_2['cleaned_tweet'] = validation_file_2['cleaned_tweet'].replace(to_replace=r'@([_A-Za-z]+[A-Za-z0-9-_]+)', value='', regex=True)\n",
    "\n",
    "# remove all other non-standard characters (emoji, #, @....) ( For this task I thought '@'' and '#' are useless )\n",
    "validation_file_2['cleaned_tweet'] = validation_file_2['cleaned_tweet'].replace(to_replace=r\"\"\"([^A-Za-z0-9 .,\\-'\"\\/?!\\\\()=])+\"\"\", value='', regex=True)\n",
    "\n",
    "# remove urls\n",
    "validation_file_2['cleaned_tweet'] = validation_file_2['cleaned_tweet'].replace(to_replace=r'http\\S+', value='', regex=True)\n",
    "\n",
    "# Expand contractions\n",
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "validation_file_2['cleaned_tweet'] = validation_file_2['cleaned_tweet'].apply(decontracted)\n",
    "\n",
    "validation_file_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As reported by the papers, different tokenizers may have different performance so I tried different tokenizers,\n",
    "# their performance should be decided by the classification results\n",
    "\n",
    "# Trial 1, used the nltk.word_tokenizer\n",
    "# import nltk\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# word_lemmatizer = WordNetLemmatizer()\n",
    "# def tokenize_lemmatize(twitter):\n",
    "#     tokens = nltk.word_tokenize(twitter)\n",
    "#     tokens_lemmetized = []\n",
    "#     for token in tokens:\n",
    "#         tokens_lemmetized.append(word_lemmatizer.lemmatize(token))\n",
    "#     lemmed_twitter = ' '.join(tokens_lemmetized)\n",
    "#     return lemmed_twitter\n",
    "\n",
    "# validation_tokenization_1 = validation_file_2\n",
    "# validation_tokenization_1['cleaned_tweet'] = validation_file_2['cleaned_tweet'].apply(tokenize_lemmatize)\n",
    "\n",
    "# validation_tokenization_1['cleaned_tweet']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for processing the validation file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>begin</th>\n",
       "      <th>end</th>\n",
       "      <th>type</th>\n",
       "      <th>extraction</th>\n",
       "      <th>drug</th>\n",
       "      <th>tweet</th>\n",
       "      <th>meddra_code</th>\n",
       "      <th>meddra_term</th>\n",
       "      <th>cleaned_tweet</th>\n",
       "      <th>tokennized_cleaned_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>332317478170546176</td>\n",
       "      <td>28.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>ADR</td>\n",
       "      <td>allergies</td>\n",
       "      <td>avelox</td>\n",
       "      <td>do you have any medication allergies? \"asthma!...</td>\n",
       "      <td>10013661.0</td>\n",
       "      <td>drug allergy</td>\n",
       "      <td>do you have any medication allergies? \"asthma!...</td>\n",
       "      <td>[do, you, have, any, medication, allergies, ?,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>347806215776116737</td>\n",
       "      <td>31.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>ADR</td>\n",
       "      <td>HURT YOUR Liver</td>\n",
       "      <td>avelox</td>\n",
       "      <td>@ashleylvivian if #avelox has hurt your liver,...</td>\n",
       "      <td>10024668.0</td>\n",
       "      <td>liver damage</td>\n",
       "      <td>if avelox has hurt your liver, avoid tylenol ...</td>\n",
       "      <td>[if, avelox, has, hurt, your, liver, ,, avoid,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>350336129817509888</td>\n",
       "      <td>48.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>ADR</td>\n",
       "      <td>AD</td>\n",
       "      <td>baclofen</td>\n",
       "      <td>apparently, baclofen greatly exacerbates the \"...</td>\n",
       "      <td>10003731.0</td>\n",
       "      <td>attention deficit disorder</td>\n",
       "      <td>apparently, baclofen greatly exacerbates the \"...</td>\n",
       "      <td>[apparently, ,, baclofen, greatly, exacerbates...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>350336129817509888</td>\n",
       "      <td>88.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>ADR</td>\n",
       "      <td>focus</td>\n",
       "      <td>baclofen</td>\n",
       "      <td>apparently, baclofen greatly exacerbates the \"...</td>\n",
       "      <td>10003738.0</td>\n",
       "      <td>attention impaired</td>\n",
       "      <td>apparently, baclofen greatly exacerbates the \"...</td>\n",
       "      <td>[apparently, ,, baclofen, greatly, exacerbates...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>332540699692130304</td>\n",
       "      <td>11.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>ADR</td>\n",
       "      <td>died</td>\n",
       "      <td>cipro</td>\n",
       "      <td>pt of mine died from cipro rt @ciproispoison: ...</td>\n",
       "      <td>10011906.0</td>\n",
       "      <td>death</td>\n",
       "      <td>pt of mine died from cipro rt   if only more d...</td>\n",
       "      <td>[pt, of, mine, died, from, cipro, rt, if, only...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id  begin   end type       extraction      drug  \\\n",
       "0  332317478170546176   28.0  37.0  ADR        allergies    avelox   \n",
       "1  347806215776116737   31.0  46.0  ADR  HURT YOUR Liver    avelox   \n",
       "2  350336129817509888   48.0  50.0  ADR               AD  baclofen   \n",
       "3  350336129817509888   88.0  93.0  ADR            focus  baclofen   \n",
       "4  332540699692130304   11.0  15.0  ADR             died     cipro   \n",
       "\n",
       "                                               tweet  meddra_code  \\\n",
       "0  do you have any medication allergies? \"asthma!...   10013661.0   \n",
       "1  @ashleylvivian if #avelox has hurt your liver,...   10024668.0   \n",
       "2  apparently, baclofen greatly exacerbates the \"...   10003731.0   \n",
       "3  apparently, baclofen greatly exacerbates the \"...   10003738.0   \n",
       "4  pt of mine died from cipro rt @ciproispoison: ...   10011906.0   \n",
       "\n",
       "                  meddra_term  \\\n",
       "0                drug allergy   \n",
       "1                liver damage   \n",
       "2  attention deficit disorder   \n",
       "3          attention impaired   \n",
       "4                       death   \n",
       "\n",
       "                                       cleaned_tweet  \\\n",
       "0  do you have any medication allergies? \"asthma!...   \n",
       "1   if avelox has hurt your liver, avoid tylenol ...   \n",
       "2  apparently, baclofen greatly exacerbates the \"...   \n",
       "3  apparently, baclofen greatly exacerbates the \"...   \n",
       "4  pt of mine died from cipro rt   if only more d...   \n",
       "\n",
       "                            tokennized_cleaned_tweet  \n",
       "0  [do, you, have, any, medication, allergies, ?,...  \n",
       "1  [if, avelox, has, hurt, your, liver, ,, avoid,...  \n",
       "2  [apparently, ,, baclofen, greatly, exacerbates...  \n",
       "3  [apparently, ,, baclofen, greatly, exacerbates...  \n",
       "4  [pt, of, mine, died, from, cipro, rt, if, only...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "validation_tokenization_2 = validation_file_2.copy()\n",
    "tokenize = nltk.word_tokenize\n",
    "validation_tokenization_2['tokennized_cleaned_tweet'] = validation_tokenization_2['cleaned_tweet'].apply(tokenize)\n",
    "validation_tokenization_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>begin</th>\n",
       "      <th>end</th>\n",
       "      <th>type</th>\n",
       "      <th>extraction</th>\n",
       "      <th>drug</th>\n",
       "      <th>tweet</th>\n",
       "      <th>meddra_code</th>\n",
       "      <th>meddra_term</th>\n",
       "      <th>cleaned_tweet</th>\n",
       "      <th>tokennized_cleaned_tweet</th>\n",
       "      <th>extraction_token</th>\n",
       "      <th>labeled_sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>332317478170546176</td>\n",
       "      <td>28</td>\n",
       "      <td>37</td>\n",
       "      <td>ADR</td>\n",
       "      <td>allergies</td>\n",
       "      <td>avelox</td>\n",
       "      <td>do you have any medication allergies? \"asthma!...</td>\n",
       "      <td>1.00137e+07</td>\n",
       "      <td>drug allergy</td>\n",
       "      <td>do you have any medication allergies? \"asthma!...</td>\n",
       "      <td>[do, you, have, any, medication, allergies, ?,...</td>\n",
       "      <td>[allergies]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>347806215776116737</td>\n",
       "      <td>31</td>\n",
       "      <td>46</td>\n",
       "      <td>ADR</td>\n",
       "      <td>HURT YOUR Liver</td>\n",
       "      <td>avelox</td>\n",
       "      <td>@ashleylvivian if #avelox has hurt your liver,...</td>\n",
       "      <td>1.00247e+07</td>\n",
       "      <td>liver damage</td>\n",
       "      <td>if avelox has hurt your liver, avoid tylenol ...</td>\n",
       "      <td>[if, avelox, has, hurt, your, liver, ,, avoid,...</td>\n",
       "      <td>[HURT, YOUR, Liver]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>350336129817509888</td>\n",
       "      <td>48</td>\n",
       "      <td>50</td>\n",
       "      <td>ADR</td>\n",
       "      <td>AD</td>\n",
       "      <td>baclofen</td>\n",
       "      <td>apparently, baclofen greatly exacerbates the \"...</td>\n",
       "      <td>1.00037e+07</td>\n",
       "      <td>attention deficit disorder</td>\n",
       "      <td>apparently, baclofen greatly exacerbates the \"...</td>\n",
       "      <td>[apparently, ,, baclofen, greatly, exacerbates...</td>\n",
       "      <td>[AD]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>350336129817509888</td>\n",
       "      <td>88</td>\n",
       "      <td>93</td>\n",
       "      <td>ADR</td>\n",
       "      <td>focus</td>\n",
       "      <td>baclofen</td>\n",
       "      <td>apparently, baclofen greatly exacerbates the \"...</td>\n",
       "      <td>1.00037e+07</td>\n",
       "      <td>attention impaired</td>\n",
       "      <td>apparently, baclofen greatly exacerbates the \"...</td>\n",
       "      <td>[apparently, ,, baclofen, greatly, exacerbates...</td>\n",
       "      <td>[focus]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>332540699692130304</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>ADR</td>\n",
       "      <td>died</td>\n",
       "      <td>cipro</td>\n",
       "      <td>pt of mine died from cipro rt @ciproispoison: ...</td>\n",
       "      <td>1.00119e+07</td>\n",
       "      <td>death</td>\n",
       "      <td>pt of mine died from cipro rt   if only more d...</td>\n",
       "      <td>[pt, of, mine, died, from, cipro, rt, if, only...</td>\n",
       "      <td>[died]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id begin end type       extraction      drug  \\\n",
       "0  332317478170546176    28  37  ADR        allergies    avelox   \n",
       "1  347806215776116737    31  46  ADR  HURT YOUR Liver    avelox   \n",
       "2  350336129817509888    48  50  ADR               AD  baclofen   \n",
       "3  350336129817509888    88  93  ADR            focus  baclofen   \n",
       "4  332540699692130304    11  15  ADR             died     cipro   \n",
       "\n",
       "                                               tweet  meddra_code  \\\n",
       "0  do you have any medication allergies? \"asthma!...  1.00137e+07   \n",
       "1  @ashleylvivian if #avelox has hurt your liver,...  1.00247e+07   \n",
       "2  apparently, baclofen greatly exacerbates the \"...  1.00037e+07   \n",
       "3  apparently, baclofen greatly exacerbates the \"...  1.00037e+07   \n",
       "4  pt of mine died from cipro rt @ciproispoison: ...  1.00119e+07   \n",
       "\n",
       "                  meddra_term  \\\n",
       "0                drug allergy   \n",
       "1                liver damage   \n",
       "2  attention deficit disorder   \n",
       "3          attention impaired   \n",
       "4                       death   \n",
       "\n",
       "                                       cleaned_tweet  \\\n",
       "0  do you have any medication allergies? \"asthma!...   \n",
       "1   if avelox has hurt your liver, avoid tylenol ...   \n",
       "2  apparently, baclofen greatly exacerbates the \"...   \n",
       "3  apparently, baclofen greatly exacerbates the \"...   \n",
       "4  pt of mine died from cipro rt   if only more d...   \n",
       "\n",
       "                            tokennized_cleaned_tweet     extraction_token  \\\n",
       "0  [do, you, have, any, medication, allergies, ?,...          [allergies]   \n",
       "1  [if, avelox, has, hurt, your, liver, ,, avoid,...  [HURT, YOUR, Liver]   \n",
       "2  [apparently, ,, baclofen, greatly, exacerbates...                 [AD]   \n",
       "3  [apparently, ,, baclofen, greatly, exacerbates...              [focus]   \n",
       "4  [pt, of, mine, died, from, cipro, rt, if, only...               [died]   \n",
       "\n",
       "  labeled_sequence  \n",
       "0                   \n",
       "1                   \n",
       "2                   \n",
       "3                   \n",
       "4                   "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fill the NAN with None \n",
    "validation_tokenization_2.fillna('None',inplace=True)\n",
    "\n",
    "# tokenize the already extracted label(AEs)\n",
    "validation_tokenization_2['extraction_token'] = validation_tokenization_2['extraction'].apply(tokenize)\n",
    "labeled_dataset = validation_tokenization_2\n",
    "\n",
    "# Create a empty column 'label' for sequence labeling \n",
    "labeled_dataset['labeled_sequence'] =''\n",
    "labeled_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>begin</th>\n",
       "      <th>end</th>\n",
       "      <th>type</th>\n",
       "      <th>extraction</th>\n",
       "      <th>drug</th>\n",
       "      <th>tweet</th>\n",
       "      <th>meddra_code</th>\n",
       "      <th>meddra_term</th>\n",
       "      <th>cleaned_tweet</th>\n",
       "      <th>tokennized_cleaned_tweet</th>\n",
       "      <th>extraction_token</th>\n",
       "      <th>labeled_sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>332317478170546176</td>\n",
       "      <td>28</td>\n",
       "      <td>37</td>\n",
       "      <td>ADR</td>\n",
       "      <td>allergies</td>\n",
       "      <td>avelox</td>\n",
       "      <td>do you have any medication allergies? \"asthma!...</td>\n",
       "      <td>1.00137e+07</td>\n",
       "      <td>drug allergy</td>\n",
       "      <td>do you have any medication allergies? \"asthma!...</td>\n",
       "      <td>[do, you, have, any, medication, allergies, ?,...</td>\n",
       "      <td>[allergies]</td>\n",
       "      <td>[O, O, O, O, O, B, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>347806215776116737</td>\n",
       "      <td>31</td>\n",
       "      <td>46</td>\n",
       "      <td>ADR</td>\n",
       "      <td>HURT YOUR Liver</td>\n",
       "      <td>avelox</td>\n",
       "      <td>@ashleylvivian if #avelox has hurt your liver,...</td>\n",
       "      <td>1.00247e+07</td>\n",
       "      <td>liver damage</td>\n",
       "      <td>if avelox has hurt your liver, avoid tylenol ...</td>\n",
       "      <td>[if, avelox, has, hurt, your, liver, ,, avoid,...</td>\n",
       "      <td>[HURT, YOUR, Liver]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>350336129817509888</td>\n",
       "      <td>48</td>\n",
       "      <td>50</td>\n",
       "      <td>ADR</td>\n",
       "      <td>AD</td>\n",
       "      <td>baclofen</td>\n",
       "      <td>apparently, baclofen greatly exacerbates the \"...</td>\n",
       "      <td>1.00037e+07</td>\n",
       "      <td>attention deficit disorder</td>\n",
       "      <td>apparently, baclofen greatly exacerbates the \"...</td>\n",
       "      <td>[apparently, ,, baclofen, greatly, exacerbates...</td>\n",
       "      <td>[AD]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>350336129817509888</td>\n",
       "      <td>88</td>\n",
       "      <td>93</td>\n",
       "      <td>ADR</td>\n",
       "      <td>focus</td>\n",
       "      <td>baclofen</td>\n",
       "      <td>apparently, baclofen greatly exacerbates the \"...</td>\n",
       "      <td>1.00037e+07</td>\n",
       "      <td>attention impaired</td>\n",
       "      <td>apparently, baclofen greatly exacerbates the \"...</td>\n",
       "      <td>[apparently, ,, baclofen, greatly, exacerbates...</td>\n",
       "      <td>[focus]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>332540699692130304</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>ADR</td>\n",
       "      <td>died</td>\n",
       "      <td>cipro</td>\n",
       "      <td>pt of mine died from cipro rt @ciproispoison: ...</td>\n",
       "      <td>1.00119e+07</td>\n",
       "      <td>death</td>\n",
       "      <td>pt of mine died from cipro rt   if only more d...</td>\n",
       "      <td>[pt, of, mine, died, from, cipro, rt, if, only...</td>\n",
       "      <td>[died]</td>\n",
       "      <td>[O, O, O, B, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id begin end type       extraction      drug  \\\n",
       "0  332317478170546176    28  37  ADR        allergies    avelox   \n",
       "1  347806215776116737    31  46  ADR  HURT YOUR Liver    avelox   \n",
       "2  350336129817509888    48  50  ADR               AD  baclofen   \n",
       "3  350336129817509888    88  93  ADR            focus  baclofen   \n",
       "4  332540699692130304    11  15  ADR             died     cipro   \n",
       "\n",
       "                                               tweet  meddra_code  \\\n",
       "0  do you have any medication allergies? \"asthma!...  1.00137e+07   \n",
       "1  @ashleylvivian if #avelox has hurt your liver,...  1.00247e+07   \n",
       "2  apparently, baclofen greatly exacerbates the \"...  1.00037e+07   \n",
       "3  apparently, baclofen greatly exacerbates the \"...  1.00037e+07   \n",
       "4  pt of mine died from cipro rt @ciproispoison: ...  1.00119e+07   \n",
       "\n",
       "                  meddra_term  \\\n",
       "0                drug allergy   \n",
       "1                liver damage   \n",
       "2  attention deficit disorder   \n",
       "3          attention impaired   \n",
       "4                       death   \n",
       "\n",
       "                                       cleaned_tweet  \\\n",
       "0  do you have any medication allergies? \"asthma!...   \n",
       "1   if avelox has hurt your liver, avoid tylenol ...   \n",
       "2  apparently, baclofen greatly exacerbates the \"...   \n",
       "3  apparently, baclofen greatly exacerbates the \"...   \n",
       "4  pt of mine died from cipro rt   if only more d...   \n",
       "\n",
       "                            tokennized_cleaned_tweet     extraction_token  \\\n",
       "0  [do, you, have, any, medication, allergies, ?,...          [allergies]   \n",
       "1  [if, avelox, has, hurt, your, liver, ,, avoid,...  [HURT, YOUR, Liver]   \n",
       "2  [apparently, ,, baclofen, greatly, exacerbates...                 [AD]   \n",
       "3  [apparently, ,, baclofen, greatly, exacerbates...              [focus]   \n",
       "4  [pt, of, mine, died, from, cipro, rt, if, only...               [died]   \n",
       "\n",
       "                                    labeled_sequence  \n",
       "0  [O, O, O, O, O, B, O, O, O, O, O, O, O, O, O, ...  \n",
       "1  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "2  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "3  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "4  [O, O, O, B, O, O, O, O, O, O, O, O, O, O, O, ...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# labeling the twitter, the label for beginign of adverse effects is \"B\", other part of ADR is 'I' and the rest of twitter are 'O'\n",
    "def sequence_check(test):\n",
    "    for i in range(1,len(test)-1):\n",
    "        if test[i-1:i+2] == ['O', 'I', 'O']:\n",
    "            test[i] = 'O'\n",
    "    return test\n",
    "def sequence_label(sequence,extraction):\n",
    "    labels = []\n",
    "    for i in sequence:\n",
    "        if i in extraction:\n",
    "            if i == extraction[0]:\n",
    "                labels.append(\"B\")\n",
    "            else:\n",
    "                labels.append(\"I\")\n",
    "        else:\n",
    "            labels.append('O')\n",
    "    sequence_check(labels)\n",
    "    return labels\n",
    "\n",
    "\n",
    "for i in range(0, len(labeled_dataset)):\n",
    "    labeled_dataset['labeled_sequence'][i] = sequence_label(labeled_dataset['tokennized_cleaned_tweet'][i],labeled_dataset['extraction_token'][i])\n",
    "\n",
    "labeled_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(560, 13)\n",
      "365\n"
     ]
    }
   ],
   "source": [
    "print(labeled_dataset.shape)\n",
    "print(sum(labeled_dataset['meddra_code']!=\"None\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/Validation_data.txt\", \"a\") as f:\n",
    "    for i in range(0,labeled_dataset.shape[0]):\n",
    "        for k in range(0,len(labeled_dataset['tokennized_cleaned_tweet'][i])):\n",
    "            line = labeled_dataset['tokennized_cleaned_tweet'][i][k] + \"\\t\"+ labeled_dataset['labeled_sequence'][i][k]\n",
    "            print(line,file=f)\n",
    "        print('\\n',file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for generating the the validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "from typing import List\n",
    "import sys\n",
    "import datetime\n",
    "import os\n",
    "import random\n",
    "import logging\n",
    "import numpy as np\n",
    "import copy\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch import autograd\n",
    "import torch.nn.functional as F\n",
    "from flair.data_fetcher import NLPTaskDataFetcher, NLPTask\n",
    "from flair.embeddings import WordEmbeddings, StackedEmbeddings\n",
    "from flair.training_utils import store_embeddings\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from model import Tagger\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(train_file, dev_file, downsample_rate = None):\n",
    "    corpus = None\n",
    "    if downsample_rate is None:\n",
    "        corpus = NLPTaskDataFetcher.load_column_corpus(\n",
    "            './', columns, train_file=train_file, dev_file=dev_file\n",
    "        )\n",
    "    else:\n",
    "        corpus = NLPTaskDataFetcher.load_column_corpus(\n",
    "           './' , columns, train_file=train_file, dev_file=dev_file\n",
    "        ).downsample(downsample_rate)\n",
    "    tag_dict = corpus.make_tag_dictionary(tag_type='tag')\n",
    "    print(corpus)\n",
    "    return corpus.train, corpus.dev, tag_dict\n",
    "            \n",
    "            \n",
    "def make_batch_feature(sentences):\n",
    "    max_len = max([len(sentence) for sentence in sentences])\n",
    "    lengths = [len(sentence.tokens) for sentence in sentences]\n",
    "    sents_tensor = torch.zeros(len(sentences), max_len, embedding_dim)\n",
    "    tag_list = []\n",
    "    for s_id, s in enumerate(sentences):\n",
    "        sents_tensor[s_id, :len(s)] = torch.cat([w.get_embedding().unsqueeze(0) for w in s], dim=0)\n",
    "        tag_idx = [tag_dict.get_idx_for_item(t.get_tag('tag').value) for t in s]\n",
    "\n",
    "        tag_idx_tensor = torch.LongTensor(tag_idx).to(device)\n",
    "        tag_list.append(tag_idx_tensor)\n",
    "        \n",
    "    sents_tensor = sents_tensor.to(device)\n",
    "    return sents_tensor, tag_list, lengths\n",
    "\n",
    "def train(sentences, use_crf=True):\n",
    "    optimizer.zero_grad()\n",
    "    word_embedding.embed(sentences)\n",
    "    feature_batch, tag_batch, lengths = make_batch_feature(sentences)\n",
    "    \n",
    "    output  = tagger(feature_batch, lengths)\n",
    "    loss_batch = 0.0\n",
    "    if use_crf:\n",
    "        loss_batch = tagger.crf.neg_log_likelihood(output, tag_batch, lengths)\n",
    "    else:\n",
    "        for i in range(len(tag_batch)):\n",
    "            sent_length = tag_batch[i].shape[0]\n",
    "            sent_feature = output[i,:sent_length, :]\n",
    "            tag_tensor = tag_batch[i]\n",
    "            loss_batch += F.cross_entropy(sent_feature, tag_tensor)\n",
    "\n",
    "    total_loss_batch = loss_batch\n",
    "    cur_loss = total_loss_batch.item()\n",
    "    store_embeddings(sentences, 'cpu')\n",
    "    total_loss_batch.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(tagger.parameters(), 5.0)\n",
    "    optimizer.step()\n",
    "    return cur_loss\n",
    "            \n",
    "def evaluate(sentences, use_crf=True):\n",
    "    all_confidences = []\n",
    "    all_pred_tags = []\n",
    "    all_gold_tags = []\n",
    "    with torch.no_grad():\n",
    "        batches = [sentences[x:x + batch_size] for x in range(0, len(sentences), batch_size)]\n",
    "        for batch_no, sent_batch in enumerate(batches):\n",
    "            word_embedding.embed(sent_batch)\n",
    "            feature_batch, tag_batch, lengths = make_batch_feature(sent_batch)\n",
    "            output = tagger(feature_batch, lengths)\n",
    "            if not use_crf:\n",
    "                softmax = F.softmax(output, dim=2)\n",
    "                for i in range(len(lengths)):\n",
    "                    sent_length = lengths[i]\n",
    "                    tag_tensor = tag_batch[i]\n",
    "                    pred_tag, confidences = [], []\n",
    "                    for j in range(sent_length):\n",
    "                        _, idx = torch.max(softmax[i, j, :], 0)\n",
    "                        pred_tag.append(idx.item())\n",
    "                        confidences.append(softmax[idx.item()])\n",
    "\n",
    "                    all_pred_tags.append([tag_dict.get_item_for_index(ix) for ix in pred_tag])\n",
    "                    all_confidences.append(confidences)\n",
    "                    all_gold_tags.append([tag_dict.get_item_for_index(int(ix)) for ix in tag_tensor.tolist()])\n",
    "            else:\n",
    "                for i in range(len(lengths)):\n",
    "                    sent_length = lengths[i]\n",
    "                    sent_feature = output[i,:sent_length, :]\n",
    "                    tag_tensor = tag_batch[i][:sent_length]\n",
    "                    confidences, pred_tag = tagger.crf.viterbi_decode(sent_feature)\n",
    "                    all_pred_tags.append([tag_dict.get_item_for_index(ix) for ix in pred_tag])\n",
    "                    all_confidences.append(confidences)\n",
    "                    all_gold_tags.append(\n",
    "                        [tag_dict.get_item_for_index(ix) for ix in tag_tensor.tolist()]\n",
    "                    )\n",
    "            store_embeddings(sentences, 'none')\n",
    "    return all_confidences, all_pred_tags, all_gold_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:model size (trainable parameters): 71542\n"
     ]
    }
   ],
   "source": [
    "tagger = Tagger(\n",
    "    in_dim=embedding_dim, relearned_dim=relearned_dim, hidden_dim=hidden_dim, \n",
    "    tag_dict=tag_dict, use_crf=use_crf,use_drop=0.0).to(device)\n",
    "    \n",
    "param_size = sum(p.numel() for p in tagger.parameters() if p.requires_grad)\n",
    "logging.info('model size (trainable parameters): {}'.format(param_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-16 16:52:14,736 Reading data from .\n",
      "2020-04-16 16:52:14,737 Train: data/Training_data.txt\n",
      "2020-04-16 16:52:14,738 Dev: data/Validation_data.txt\n",
      "2020-04-16 16:52:14,738 Test: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated function (or staticmethod) load_column_corpus. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "  \"\"\"\n",
      "INFO:gensim.utils:loading Word2VecKeyedVectors object from /Users/jonathanmartindale/.flair/embeddings/glove.gensim\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus: 2021 train + 1120 dev + 225 test sentences\n",
      "{b'<unk>': 0, b'O': 1, b'B': 2, b'I': 3, b'<START>': 4, b'<STOP>': 5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.utils:loading vectors from /Users/jonathanmartindale/.flair/embeddings/glove.gensim.vectors.npy with mmap=None\n",
      "INFO:gensim.utils:setting ignored attribute vectors_norm to None\n",
      "INFO:gensim.utils:loaded /Users/jonathanmartindale/.flair/embeddings/glove.gensim\n"
     ]
    }
   ],
   "source": [
    "train_file = 'data/Training_data.txt' # path to train data\n",
    "dev_file = 'data/Validation_data.txt'  # path to validation data\n",
    "\n",
    "model_output = 'sample-model.pt'\n",
    "\n",
    "label_list = ['B' , 'I', 'O']\n",
    "\n",
    "tag_type = 'tag'\n",
    "columns = {0: 'text', 1: 'tag'}\n",
    "train_data, dev_data, tag_dict = load_data(\n",
    "    train_file=train_file, dev_file=dev_file,\n",
    "    downsample_rate=None # using a downsample rate can run small sample data size for fast code testing!!!\n",
    ")\n",
    "train_data, dev_data = list(train_data), list(dev_data)\n",
    "\n",
    "print(tag_dict.item2idx)\n",
    "emb1 = WordEmbeddings('glove')  # create embedings you want use.\n",
    "\n",
    "emb_types = [emb1] # if you have multiple types of embeddings, just add them here in the list.\n",
    "word_embedding = StackedEmbeddings(embeddings=emb_types)\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "use_crf = True\n",
    "embedding_dim = 100  # embedding dim need to match the embedding size you choose to use. \n",
    "relearned_dim = embedding_dim\n",
    "hidden_dim=50\n",
    "lr = 0.001\n",
    "optimizer = torch.optim.SGD(tagger.parameters(), lr=lr)\n",
    "training_epoch =10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:epoch 1 - iter 0/64 - lr 0.001 - loss 2.296811\n",
      "INFO:root:epoch 1 - iter 6/64 - lr 0.001 - loss 2.585885\n",
      "INFO:root:epoch 1 - iter 12/64 - lr 0.001 - loss 2.612047\n",
      "INFO:root:epoch 1 - iter 18/64 - lr 0.001 - loss 2.599777\n",
      "INFO:root:epoch 1 - iter 24/64 - lr 0.001 - loss 2.558846\n",
      "INFO:root:epoch 1 - iter 30/64 - lr 0.001 - loss 2.514242\n",
      "INFO:root:epoch 1 - iter 36/64 - lr 0.001 - loss 2.473118\n",
      "INFO:root:epoch 1 - iter 42/64 - lr 0.001 - loss 2.443766\n",
      "INFO:root:epoch 1 - iter 48/64 - lr 0.001 - loss 2.413973\n",
      "INFO:root:epoch 1 - iter 54/64 - lr 0.001 - loss 2.394863\n",
      "INFO:root:epoch 1 - iter 60/64 - lr 0.001 - loss 2.385889\n",
      "INFO:root:----------------------- Evaluate DEV set -----------------------\n",
      "INFO:root:epoch 2 - iter 0/64 - lr 0.001 - loss 2.196755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           B       0.00      0.00      0.00       666\n",
      "           I       0.03      0.47      0.05       700\n",
      "           O       0.94      0.51      0.66     22866\n",
      "\n",
      "    accuracy                           0.50     24232\n",
      "   macro avg       0.33      0.33      0.24     24232\n",
      "weighted avg       0.89      0.50      0.63     24232\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:epoch 2 - iter 6/64 - lr 0.001 - loss 2.189447\n",
      "INFO:root:epoch 2 - iter 12/64 - lr 0.001 - loss 2.194093\n",
      "INFO:root:epoch 2 - iter 18/64 - lr 0.001 - loss 2.178160\n",
      "INFO:root:epoch 2 - iter 24/64 - lr 0.001 - loss 2.156801\n",
      "INFO:root:epoch 2 - iter 30/64 - lr 0.001 - loss 2.152799\n",
      "INFO:root:epoch 2 - iter 36/64 - lr 0.001 - loss 2.124750\n",
      "INFO:root:epoch 2 - iter 42/64 - lr 0.001 - loss 2.093147\n",
      "INFO:root:epoch 2 - iter 48/64 - lr 0.001 - loss 2.081180\n",
      "INFO:root:epoch 2 - iter 54/64 - lr 0.001 - loss 2.067679\n",
      "INFO:root:epoch 2 - iter 60/64 - lr 0.001 - loss 2.054255\n",
      "INFO:root:----------------------- Evaluate DEV set -----------------------\n",
      "INFO:root:epoch 3 - iter 0/64 - lr 0.001 - loss 1.904067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           B       0.00      0.00      0.00       666\n",
      "           I       0.03      0.47      0.06       700\n",
      "           O       0.94      0.51      0.66     22866\n",
      "\n",
      "    accuracy                           0.50     24232\n",
      "   macro avg       0.33      0.33      0.24     24232\n",
      "weighted avg       0.89      0.50      0.63     24232\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:epoch 3 - iter 6/64 - lr 0.001 - loss 1.887170\n",
      "INFO:root:epoch 3 - iter 12/64 - lr 0.001 - loss 1.810768\n",
      "INFO:root:epoch 3 - iter 18/64 - lr 0.001 - loss 1.796153\n",
      "INFO:root:epoch 3 - iter 24/64 - lr 0.001 - loss 1.807050\n",
      "INFO:root:epoch 3 - iter 30/64 - lr 0.001 - loss 1.793567\n",
      "INFO:root:epoch 3 - iter 36/64 - lr 0.001 - loss 1.779093\n",
      "INFO:root:epoch 3 - iter 42/64 - lr 0.001 - loss 1.771151\n",
      "INFO:root:epoch 3 - iter 48/64 - lr 0.001 - loss 1.761277\n",
      "INFO:root:epoch 3 - iter 54/64 - lr 0.001 - loss 1.744906\n",
      "INFO:root:epoch 3 - iter 60/64 - lr 0.001 - loss 1.721840\n",
      "INFO:root:----------------------- Evaluate DEV set -----------------------\n",
      "INFO:root:epoch 4 - iter 0/64 - lr 0.001 - loss 1.548888\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           B       0.00      0.00      0.00       666\n",
      "           I       0.03      0.47      0.05       700\n",
      "           O       0.95      0.51      0.66     22866\n",
      "\n",
      "    accuracy                           0.50     24232\n",
      "   macro avg       0.33      0.33      0.24     24232\n",
      "weighted avg       0.89      0.50      0.63     24232\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:epoch 4 - iter 6/64 - lr 0.001 - loss 1.516894\n",
      "INFO:root:epoch 4 - iter 12/64 - lr 0.001 - loss 1.481447\n",
      "INFO:root:epoch 4 - iter 18/64 - lr 0.001 - loss 1.475399\n",
      "INFO:root:epoch 4 - iter 24/64 - lr 0.001 - loss 1.470049\n",
      "INFO:root:epoch 4 - iter 30/64 - lr 0.001 - loss 1.447426\n",
      "INFO:root:epoch 4 - iter 36/64 - lr 0.001 - loss 1.429865\n",
      "INFO:root:epoch 4 - iter 42/64 - lr 0.001 - loss 1.420563\n",
      "INFO:root:epoch 4 - iter 48/64 - lr 0.001 - loss 1.408418\n",
      "INFO:root:epoch 4 - iter 54/64 - lr 0.001 - loss 1.386953\n",
      "INFO:root:epoch 4 - iter 60/64 - lr 0.001 - loss 1.371205\n",
      "INFO:root:----------------------- Evaluate DEV set -----------------------\n",
      "INFO:root:epoch 5 - iter 0/64 - lr 0.001 - loss 1.153255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           B       0.00      0.00      0.00       666\n",
      "           I       0.03      0.47      0.06       700\n",
      "           O       0.95      0.52      0.67     22866\n",
      "\n",
      "    accuracy                           0.51     24232\n",
      "   macro avg       0.33      0.33      0.24     24232\n",
      "weighted avg       0.90      0.51      0.64     24232\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:epoch 5 - iter 6/64 - lr 0.001 - loss 1.189467\n",
      "INFO:root:epoch 5 - iter 12/64 - lr 0.001 - loss 1.142570\n",
      "INFO:root:epoch 5 - iter 18/64 - lr 0.001 - loss 1.133131\n",
      "INFO:root:epoch 5 - iter 24/64 - lr 0.001 - loss 1.118973\n",
      "INFO:root:epoch 5 - iter 30/64 - lr 0.001 - loss 1.105829\n",
      "INFO:root:epoch 5 - iter 36/64 - lr 0.001 - loss 1.085288\n",
      "INFO:root:epoch 5 - iter 42/64 - lr 0.001 - loss 1.068281\n",
      "INFO:root:epoch 5 - iter 48/64 - lr 0.001 - loss 1.050989\n",
      "INFO:root:epoch 5 - iter 54/64 - lr 0.001 - loss 1.033107\n",
      "INFO:root:epoch 5 - iter 60/64 - lr 0.001 - loss 1.011347\n",
      "INFO:root:----------------------- Evaluate DEV set -----------------------\n",
      "INFO:root:epoch 6 - iter 0/64 - lr 0.001 - loss 0.901514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           B       0.00      0.00      0.00       666\n",
      "           I       0.03      0.47      0.06       700\n",
      "           O       0.95      0.55      0.69     22866\n",
      "\n",
      "    accuracy                           0.53     24232\n",
      "   macro avg       0.33      0.34      0.25     24232\n",
      "weighted avg       0.90      0.53      0.66     24232\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:epoch 6 - iter 6/64 - lr 0.001 - loss 0.791912\n",
      "INFO:root:epoch 6 - iter 12/64 - lr 0.001 - loss 0.784135\n",
      "INFO:root:epoch 6 - iter 18/64 - lr 0.001 - loss 0.778692\n",
      "INFO:root:epoch 6 - iter 24/64 - lr 0.001 - loss 0.754296\n",
      "INFO:root:epoch 6 - iter 30/64 - lr 0.001 - loss 0.733340\n",
      "INFO:root:epoch 6 - iter 36/64 - lr 0.001 - loss 0.717377\n",
      "INFO:root:epoch 6 - iter 42/64 - lr 0.001 - loss 0.700777\n",
      "INFO:root:epoch 6 - iter 48/64 - lr 0.001 - loss 0.689825\n",
      "INFO:root:epoch 6 - iter 54/64 - lr 0.001 - loss 0.678810\n",
      "INFO:root:epoch 6 - iter 60/64 - lr 0.001 - loss 0.665486\n",
      "INFO:root:----------------------- Evaluate DEV set -----------------------\n",
      "INFO:root:epoch 7 - iter 0/64 - lr 0.001 - loss 0.435701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           B       0.00      0.00      0.00       666\n",
      "           I       0.03      0.24      0.06       700\n",
      "           O       0.95      0.79      0.87     22866\n",
      "\n",
      "    accuracy                           0.76     24232\n",
      "   macro avg       0.33      0.35      0.31     24232\n",
      "weighted avg       0.90      0.76      0.82     24232\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:epoch 7 - iter 6/64 - lr 0.001 - loss 0.516353\n",
      "INFO:root:epoch 7 - iter 12/64 - lr 0.001 - loss 0.484391\n",
      "INFO:root:epoch 7 - iter 18/64 - lr 0.001 - loss 0.462261\n",
      "INFO:root:epoch 7 - iter 24/64 - lr 0.001 - loss 0.454506\n",
      "INFO:root:epoch 7 - iter 30/64 - lr 0.001 - loss 0.445299\n",
      "INFO:root:epoch 7 - iter 36/64 - lr 0.001 - loss 0.437990\n",
      "INFO:root:epoch 7 - iter 42/64 - lr 0.001 - loss 0.428016\n",
      "INFO:root:epoch 7 - iter 48/64 - lr 0.001 - loss 0.418712\n",
      "INFO:root:epoch 7 - iter 54/64 - lr 0.001 - loss 0.411626\n",
      "INFO:root:epoch 7 - iter 60/64 - lr 0.001 - loss 0.405179\n",
      "INFO:root:----------------------- Evaluate DEV set -----------------------\n",
      "INFO:root:epoch 8 - iter 0/64 - lr 0.001 - loss 0.306551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           B       0.00      0.00      0.00       666\n",
      "           I       0.02      0.04      0.02       700\n",
      "           O       0.94      0.93      0.94     22866\n",
      "\n",
      "    accuracy                           0.88     24232\n",
      "   macro avg       0.32      0.32      0.32     24232\n",
      "weighted avg       0.89      0.88      0.88     24232\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:epoch 8 - iter 6/64 - lr 0.001 - loss 0.309499\n",
      "INFO:root:epoch 8 - iter 12/64 - lr 0.001 - loss 0.310398\n",
      "INFO:root:epoch 8 - iter 18/64 - lr 0.001 - loss 0.310386\n",
      "INFO:root:epoch 8 - iter 24/64 - lr 0.001 - loss 0.300969\n",
      "INFO:root:epoch 8 - iter 30/64 - lr 0.001 - loss 0.294472\n",
      "INFO:root:epoch 8 - iter 36/64 - lr 0.001 - loss 0.300995\n",
      "INFO:root:epoch 8 - iter 42/64 - lr 0.001 - loss 0.294698\n",
      "INFO:root:epoch 8 - iter 48/64 - lr 0.001 - loss 0.289950\n",
      "INFO:root:epoch 8 - iter 54/64 - lr 0.001 - loss 0.282846\n",
      "INFO:root:epoch 8 - iter 60/64 - lr 0.001 - loss 0.280123\n",
      "INFO:root:----------------------- Evaluate DEV set -----------------------\n",
      "INFO:root:epoch 9 - iter 0/64 - lr 0.001 - loss 0.273413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           B       0.00      0.00      0.00       666\n",
      "           I       0.00      0.00      0.00       700\n",
      "           O       0.94      0.99      0.96     22866\n",
      "\n",
      "    accuracy                           0.93     24232\n",
      "   macro avg       0.31      0.33      0.32     24232\n",
      "weighted avg       0.89      0.93      0.91     24232\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:epoch 9 - iter 6/64 - lr 0.001 - loss 0.250730\n",
      "INFO:root:epoch 9 - iter 12/64 - lr 0.001 - loss 0.241038\n",
      "INFO:root:epoch 9 - iter 18/64 - lr 0.001 - loss 0.257493\n",
      "INFO:root:epoch 9 - iter 24/64 - lr 0.001 - loss 0.255746\n",
      "INFO:root:epoch 9 - iter 30/64 - lr 0.001 - loss 0.257718\n",
      "INFO:root:epoch 9 - iter 36/64 - lr 0.001 - loss 0.253844\n",
      "INFO:root:epoch 9 - iter 42/64 - lr 0.001 - loss 0.252046\n",
      "INFO:root:epoch 9 - iter 48/64 - lr 0.001 - loss 0.249835\n",
      "INFO:root:epoch 9 - iter 54/64 - lr 0.001 - loss 0.249897\n",
      "INFO:root:epoch 9 - iter 60/64 - lr 0.001 - loss 0.249378\n",
      "INFO:root:----------------------- Evaluate DEV set -----------------------\n",
      "INFO:root:epoch 10 - iter 0/64 - lr 0.001 - loss 0.184081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           B       0.00      0.00      0.00       666\n",
      "           I       0.00      0.00      0.00       700\n",
      "           O       0.94      0.99      0.97     22866\n",
      "\n",
      "    accuracy                           0.94     24232\n",
      "   macro avg       0.31      0.33      0.32     24232\n",
      "weighted avg       0.89      0.94      0.91     24232\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:epoch 10 - iter 6/64 - lr 0.001 - loss 0.227476\n",
      "INFO:root:epoch 10 - iter 12/64 - lr 0.001 - loss 0.230748\n",
      "INFO:root:epoch 10 - iter 18/64 - lr 0.001 - loss 0.241513\n",
      "INFO:root:epoch 10 - iter 24/64 - lr 0.001 - loss 0.250231\n",
      "INFO:root:epoch 10 - iter 30/64 - lr 0.001 - loss 0.244762\n",
      "INFO:root:epoch 10 - iter 36/64 - lr 0.001 - loss 0.241352\n",
      "INFO:root:epoch 10 - iter 42/64 - lr 0.001 - loss 0.242827\n",
      "INFO:root:epoch 10 - iter 48/64 - lr 0.001 - loss 0.239149\n",
      "INFO:root:epoch 10 - iter 54/64 - lr 0.001 - loss 0.239691\n",
      "INFO:root:epoch 10 - iter 60/64 - lr 0.001 - loss 0.239906\n",
      "INFO:root:----------------------- Evaluate DEV set -----------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           B       0.00      0.00      0.00       666\n",
      "           I       0.00      0.00      0.00       700\n",
      "           O       0.94      1.00      0.97     22866\n",
      "\n",
      "    accuracy                           0.94     24232\n",
      "   macro avg       0.31      0.33      0.32     24232\n",
      "weighted avg       0.89      0.94      0.91     24232\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    for epoch in range(training_epoch):\n",
    "        random.shuffle(train_data)\n",
    "        batches = [train_data[x:x + batch_size] for x in range(0, len(train_data), batch_size)]\n",
    "        tagger.train()\n",
    "        current_loss, seen_sentences, modulo = 0.0, 0, max(1, int(len(batches) / 10))\n",
    "\n",
    "        for batch_no, sent_batch in enumerate(batches):\n",
    "            loss_batch = train(sent_batch, use_crf=use_crf)\n",
    "            current_loss += (loss_batch)\n",
    "            seen_sentences += len(sent_batch)\n",
    "            if batch_no % modulo == 0:\n",
    "                logging.info(\n",
    "                    \"epoch {0} - iter {1}/{2} - lr {3} - loss {4:.6f}\".format(\n",
    "                        epoch + 1, batch_no, len(batches), lr, current_loss / seen_sentences\n",
    "                    )\n",
    "                )\n",
    "                iteration = epoch * len(batches) + batch_no\n",
    "        current_loss /= len(train_data)\n",
    "\n",
    "        tagger.eval()\n",
    "        logging.info('----------------------- Evaluate DEV set -----------------------')\n",
    "        scores, all_pred, all_true = evaluate(dev_data, use_crf=use_crf)\n",
    "        \n",
    "        all_pred_flat= [x for sent in all_pred for x in sent]\n",
    "        all_true_flat= [x for sent in all_true for x in sent] \n",
    "        print(classification_report(y_pred=all_pred_flat, y_true=all_true_flat, labels=label_list))\n",
    "        \n",
    "\n",
    "    with torch.no_grad():\n",
    "        torch.save(tagger.state_dict(), model_output)\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    print('key interepted (early stopping)')\n",
    "    with torch.no_grad():\n",
    "        torch.save(tagger.state_dict(), model_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tagger(\n",
       "  (embedding2nn): Linear(in_features=100, out_features=100, bias=True)\n",
       "  (rnn): LSTM(100, 50, batch_first=True, bidirectional=True)\n",
       "  (ffn): Linear(in_features=100, out_features=6, bias=True)\n",
       "  (crf): CRF()\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_list_to_eval = dev_data[:560]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, all_pred, all_true = evaluate(example_list_to_eval, use_crf=use_crf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: \"do you have any medication allergies ? `` asthma ! ! ! '' me `` ... ... .. '' pt `` no wait . avelox , that is it ! '' `` so no other allergies ? '' `` right ! '' cont\" - 43 Tokens\n",
      "Sentence: \"do you have any medication allergies ? `` asthma ! ! ! '' me `` ... ... .. '' pt `` no wait . avelox , that is it ! '' `` so no other allergies ? '' `` right ! '' cont\" - 43 Tokens\n"
     ]
    }
   ],
   "source": [
    "print(dev_data[0])\n",
    "print(dev_data[560])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pred_arry = np.asarray(all_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>begin</th>\n",
       "      <th>end</th>\n",
       "      <th>type</th>\n",
       "      <th>extraction</th>\n",
       "      <th>drug</th>\n",
       "      <th>tweet</th>\n",
       "      <th>meddra_code</th>\n",
       "      <th>meddra_term</th>\n",
       "      <th>cleaned_tweet</th>\n",
       "      <th>tokennized_cleaned_tweet</th>\n",
       "      <th>extraction_token</th>\n",
       "      <th>labeled_sequence</th>\n",
       "      <th>predicted_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>332317478170546176</td>\n",
       "      <td>28</td>\n",
       "      <td>37</td>\n",
       "      <td>ADR</td>\n",
       "      <td>allergies</td>\n",
       "      <td>avelox</td>\n",
       "      <td>do you have any medication allergies? \"asthma!...</td>\n",
       "      <td>1.00137e+07</td>\n",
       "      <td>drug allergy</td>\n",
       "      <td>do you have any medication allergies? \"asthma!...</td>\n",
       "      <td>[do, you, have, any, medication, allergies, ?,...</td>\n",
       "      <td>[allergies]</td>\n",
       "      <td>[O, O, O, O, O, B, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>347806215776116737</td>\n",
       "      <td>31</td>\n",
       "      <td>46</td>\n",
       "      <td>ADR</td>\n",
       "      <td>HURT YOUR Liver</td>\n",
       "      <td>avelox</td>\n",
       "      <td>@ashleylvivian if #avelox has hurt your liver,...</td>\n",
       "      <td>1.00247e+07</td>\n",
       "      <td>liver damage</td>\n",
       "      <td>if avelox has hurt your liver, avoid tylenol ...</td>\n",
       "      <td>[if, avelox, has, hurt, your, liver, ,, avoid,...</td>\n",
       "      <td>[HURT, YOUR, Liver]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>350336129817509888</td>\n",
       "      <td>48</td>\n",
       "      <td>50</td>\n",
       "      <td>ADR</td>\n",
       "      <td>AD</td>\n",
       "      <td>baclofen</td>\n",
       "      <td>apparently, baclofen greatly exacerbates the \"...</td>\n",
       "      <td>1.00037e+07</td>\n",
       "      <td>attention deficit disorder</td>\n",
       "      <td>apparently, baclofen greatly exacerbates the \"...</td>\n",
       "      <td>[apparently, ,, baclofen, greatly, exacerbates...</td>\n",
       "      <td>[AD]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>350336129817509888</td>\n",
       "      <td>88</td>\n",
       "      <td>93</td>\n",
       "      <td>ADR</td>\n",
       "      <td>focus</td>\n",
       "      <td>baclofen</td>\n",
       "      <td>apparently, baclofen greatly exacerbates the \"...</td>\n",
       "      <td>1.00037e+07</td>\n",
       "      <td>attention impaired</td>\n",
       "      <td>apparently, baclofen greatly exacerbates the \"...</td>\n",
       "      <td>[apparently, ,, baclofen, greatly, exacerbates...</td>\n",
       "      <td>[focus]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>332540699692130304</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>ADR</td>\n",
       "      <td>died</td>\n",
       "      <td>cipro</td>\n",
       "      <td>pt of mine died from cipro rt @ciproispoison: ...</td>\n",
       "      <td>1.00119e+07</td>\n",
       "      <td>death</td>\n",
       "      <td>pt of mine died from cipro rt   if only more d...</td>\n",
       "      <td>[pt, of, mine, died, from, cipro, rt, if, only...</td>\n",
       "      <td>[died]</td>\n",
       "      <td>[O, O, O, B, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id begin end type       extraction      drug  \\\n",
       "0  332317478170546176    28  37  ADR        allergies    avelox   \n",
       "1  347806215776116737    31  46  ADR  HURT YOUR Liver    avelox   \n",
       "2  350336129817509888    48  50  ADR               AD  baclofen   \n",
       "3  350336129817509888    88  93  ADR            focus  baclofen   \n",
       "4  332540699692130304    11  15  ADR             died     cipro   \n",
       "\n",
       "                                               tweet  meddra_code  \\\n",
       "0  do you have any medication allergies? \"asthma!...  1.00137e+07   \n",
       "1  @ashleylvivian if #avelox has hurt your liver,...  1.00247e+07   \n",
       "2  apparently, baclofen greatly exacerbates the \"...  1.00037e+07   \n",
       "3  apparently, baclofen greatly exacerbates the \"...  1.00037e+07   \n",
       "4  pt of mine died from cipro rt @ciproispoison: ...  1.00119e+07   \n",
       "\n",
       "                  meddra_term  \\\n",
       "0                drug allergy   \n",
       "1                liver damage   \n",
       "2  attention deficit disorder   \n",
       "3          attention impaired   \n",
       "4                       death   \n",
       "\n",
       "                                       cleaned_tweet  \\\n",
       "0  do you have any medication allergies? \"asthma!...   \n",
       "1   if avelox has hurt your liver, avoid tylenol ...   \n",
       "2  apparently, baclofen greatly exacerbates the \"...   \n",
       "3  apparently, baclofen greatly exacerbates the \"...   \n",
       "4  pt of mine died from cipro rt   if only more d...   \n",
       "\n",
       "                            tokennized_cleaned_tweet     extraction_token  \\\n",
       "0  [do, you, have, any, medication, allergies, ?,...          [allergies]   \n",
       "1  [if, avelox, has, hurt, your, liver, ,, avoid,...  [HURT, YOUR, Liver]   \n",
       "2  [apparently, ,, baclofen, greatly, exacerbates...                 [AD]   \n",
       "3  [apparently, ,, baclofen, greatly, exacerbates...              [focus]   \n",
       "4  [pt, of, mine, died, from, cipro, rt, if, only...               [died]   \n",
       "\n",
       "                                    labeled_sequence  \\\n",
       "0  [O, O, O, O, O, B, O, O, O, O, O, O, O, O, O, ...   \n",
       "1  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "2  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "3  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "4  [O, O, O, B, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "\n",
       "                                    predicted_labels  \n",
       "0  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "1  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "2  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "3  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "4  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_dataset['predicted_labels'] = all_pred_arry\n",
    "\n",
    "# for i in range(0, len(labeled_dataset)):\n",
    "#     labeled_dataset['predicted_labels'][i] = all_pred[i]\n",
    "\n",
    "labeled_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_dataset.to_csv('data/validation_file_with_predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
